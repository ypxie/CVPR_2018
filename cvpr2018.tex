\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{tabularx} % in the preamble
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcommand\VRule[1][\arrayrulewidth]{\vrule width #1}
\usepackage{array,booktabs,arydshln,xcolor} % for widening table line
\usepackage{amsthm,bm}
\usepackage{epstopdf}


\usepackage{amsthm}
\usepackage{multirow}

\usepackage{float}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{capt-of}% or \usepackage{caption}
\usepackage{booktabs}
\usepackage{varwidth}

\usepackage{algpseudocode,algorithm,algorithmicx}
\newcommand*\DNA{\textsc{dna}}

\newcommand*\Let[2]{\State #1 $\gets$ #2}
\algrenewcommand\algorithmicrequire{\textbf{Precondition:}}
\algrenewcommand\algorithmicensure{\textbf{Postcondition:}}


\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

%\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Semantic Photographic Image Synthesis \\ with a Hierarchically-nested Adversarial Network}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
This paper presents a novel  and effective approach to deal with the challenging task of generating synthetic photographic image conditioned on semantic image descriptions. We leverage the hierarchical deep representations of convolutional layers and introduce deeply-nested adversarial training to constrain the representations to be compositional for multi-scale image generation. To this end, we present a new network architecture, which is symmetric as well as extensile, to gradually upscale generated images to high resolutions.  We present a  functional generative adversarial networks (GANs) training strategy to encourage more effective multimodal (i.e. image and text) information utilization in order to enhance multi-purpose adversary between real and fake. Our method, at the first time, shows a fully end-to-end trainable network that can generate $256{\times}256$ images and is stackable to $512{\times}512$ resolution or higher with arbitrary extensions. With extensive experimental validation on three major datasets, our method sets up a new state of the art to this field. 

\end{abstract}


%%%%%%%%% BODY TEXT
\section{Introduction}
Generating photographic high-resolution image conditional on arbitrary semantic description is a meaningful problem in generative model research \cite{reed2016generative}. However, insufficient methods have been successfully developed to address this task due to its particular challenges. Text-to-image synthesis can be viewed as an special variation of conventional generative models, which aim to translate a vector to a RGB image space.  Differently, the vector space of the former is induced by semantic descriptions. Therefore, this task requires the generated images to be semantically consistent, i.e., the generated images not only reflect the principal concept of descriptions in global images but also can reflect the fine-grained descriptive details in image pixels. 

Generative adversarial networks (GANs) have become the main solution to this task. 
Reed \etal \cite{reed2016generative} takes the first step to address this task through a GAN based framework. But this method only handles image up to $64{\times}64$ resolution and usually is barely to generate vivid object details.
Based on \cite{reed2016generative}, Zhang \etal \cite{han2017stackganan} present a successful approach (StackGAN) by stacking another GAN to generate high quality and compelling $256{\times}256$ images given inputs of low resolution $64{\times}64$ synthetic images. This method needs individual two-stage training, which is relatively not straightforward to be applied. Later on, Dong \etal \ref{dong2017semantic} 
bypasses the difficult of translate vector semantic vector to RGB image and treat it as an pixel-to-pixel translation \cite{isola2016image}, by re-renders a arbitrary-style training image according to targeting semantic descriptions. However, the high resolution synthesis ability is still indeterminate. 
Unfortunately, directly training from vector space and synthesize high resolution (e.g. $256{\times}256$)  images is still an open question according to current reported studies. 

We outline several empirical reasons for this challenges using GANs. The first fundamental problem comes from the difficulty of balancing the stability between generators and discriminators. There are many a rich number of literature trying to stabilize GAN training \cite{salimans2016improved}. But especially for text-to-image synthesis, how to utilize the multimodal information between images and text in discriminator is extremely important and needs careful consideration. The second reason is that the pixel space in high resolution image is substantially large \cite{han2017stackgan}. Keeping semantic consistency as well as diversity in generated images needs more specialized designs for discriminators. 

With careful consideration of these reasons, in this paper, we propose a novel method that can directly generate high resolution images trained in fully end-to-end fashion. The contributions are described follows.

To tackle the problem of significant space difference between text and images, we propose to leverage the hierarchical deep representations of convolutional layers through deeply compositional constraints. In other words, we limit feature maps in different layers to be disentangled RGB image space, such that feature maps of any layers can linearly represent targeting images at corresponding resolutions. We introduce accompanying deeply-nested adversarial objective at intermediate layers to achieve this goal, inspired by the deeply-supervised nets (DSN) \cite{lee2015deeply} for image classification.
To well cooperate with discriminators, we propose an intuitive network architecture design with hierarchically-nested adversarial objectives that is more efficient and faster to train.

To tackle the instability training of GANs, \textcolor{red}{we propose to enforce discriminators at different resolutions can differentiate real/fake image, correct/incorrect image-text pairs, and real/fake image pairs jointly with multiple losses.} We will show that this multi-purpose strategy is mutually beneficially to encourage them only focus on respect duty. It is a regularization to limit the discriminators to use unexpected information to make decision. 

We have validated our proposed method on three datasets, i.e. CUB birds, Oxford-102 flowers, and large-scale COCO datasets. Extensive experimental results demonstrate the effectiveness of our method and significantly improved performance compared against previous state of the arts.


\section{Related Work}
There are substantial existing methods investigating better usage of GANs \cite{goodfellow2014generative,radford2015unsupervised} for different applications, such as image synthesis \cite{shrivastava2016learning}, (unpaired) pixel-to-pixel translation \cite{isola2016image,zhu2017unpaired},  medical imaging \cite{costa2017towards}, etc \cite{}.

Text-to-image synthesis is one of the most challenging tasks requires accurate semantically consistent mapping from a text vector space to a much higher image space.  Reed \etal \cite{reed2016generative} introduces the first method that can generate $64{\times}64$ resolution images, which is similar with DCGAN \cite{}. This method presents a new strategy to image-text matching aware adversarial training. Reed \etal \cite{reed2016learning} propose  generative
adversarial what-where network (GAWWN) to enable "where and what" instructions in text-to-image synthesis, which uses extra information to help generate $128{\times}128$ resolution images. StackGAN \etal \cite{han2017stackgan} propose to a two-stage stacking GAN training approach that is able to generate $256{\times}256$ high resolution vivid images. Recently, \cite{dong2017semantic} proposes to learn a joint embedding of images and text so as to re-render a prototype image according to targeting descriptions. 	
	
GANs suffer from optimization difficulties. Wide methods have been proposed to stabilize the training process, for example, one-side label smoothing and discriminator feature matching \cite{salimans2016improved}, training stronger discriminator is beneficial to encourage generators \cite{arjovsky2017wasserstein}. Balancing through a equilibrium enforcing method \cite{berthelot2017began}
Preventing discriminators from forgetting past samples and re-introduces artifacts \cite{shrivastava2016learning}. Increasing the information richness in GANs has been shown very useful \cite{odena2016conditional}. 

Leveraging hierarchical representations of neural networks is an effective way to enhance implicit multi-scaling and ensembling for tasks like image classification \cite{lee2015deeply} and edge detection \cite{xie2015holistically}. DSN \cite{lee2015deeply} introduces deep supervision in hierarchical convolutional layers to increase the discriminativenss of feature representations. In aspect of unsupervised learning, Denton \etal \cite{denton2015deep} trains a cascade of GANs within a Laplacian pyramid framework (LAPGAN) and use each to generate difference images conditioned on noise and images from last pyramid, and gradually increase image resolution. StackGAN also shares similar strategy with LAPGAN to increase output resolution. Different from these methods, 
 

deep supervision 








\subsection{Summarize different architectures}
refer to HED.

\section{Preliminary}
\subsection{Generative Adversary Network}
Generative adversary networks (GANs) are composed of a generator $G$ and a discriminator $D$, which are alternatively trained to compete with each other in a two-player minimax game. The discriminator $D$ is optimized to distinguish the synthesized images from the real images, meanwhile, the generator $G$ is trained to fool the discriminator by trying to convert an input distribution $p_z$ to the true data distribution $p_{data}$. Concretely, the overall training procedure can be analogous to the following two-player min-max game $V(D, G)$,
 \begin{equation}
 \label{game}
 \begin{split}
  \underset{G}{\min}\ \underset{D}{\max}\ V(D, G) =  &\mathbb{E}_{x\sim p_{data}}[log D(x)] +  \\
					  &\mathbb{E}_{z\sim p_{z}}[log (1-D(G(z)))]		   
 \end{split}
 \end{equation}
where $x$ is a real image sampled from the true data distribution $p_{data}$, $z$ is a noise vector usually sampled from some predefined distribution $p_{z}$ (e.g., Gaussian distribution).
In practice, It's been shown to work better for the generator $G$ to minimizing $-log(D(G(z)))$ rather than $log(1-D(G(z)))$, namely, the $log(D)$ trick.

Conditioned GAN \cite{isola2016image} is a variation of traditional GAN where both the generator network $G$ and discriminator network $D$ are conditioned on an additional condition vector besides the noise vector. In this paper, we consider the task of training a deep  generative adversarial network conditioned on the embedding of the text descriptions of the images.



\section{Method}
To generate high-resolution photo-realistic images from text embeddings, we propose a simple and effective multi-level Generative Adversarial Network architecture. Our model mainly consists of the following key-components,

\textbf{Multi-level side output:} instead of only predicting the high resolution in the final output, our model is designed to predict multi-level images of gradually increasing dimension (e.g., 256$\times$256 as the final output with 64$\times$64 and 128$\times$128 being the side output) as the side outputs. In principle, the lower-resolution side output is forced to learn the rough structure (e.g., object sketch, color information and background) of the images, the subsequent higher-resolution path is used to add more details to the images. The lower-level side output works by regularizing the lower-level image feature. Since the model is trained in an end-to-end fashion, we can observe more consistent information exhibited in both lower and higher-resolution outputs than stackGan\cite{han2017stackgan}. This type of side output is also proven useful because it allows short path between image supervision information to the hidden feature maps, which could potentially helps training.

\textbf{Component loss: } another  key component of our method is the Component Loss. conventional GAN only needs to differentiate fake images and real images. However, conditional GAN usually requires that the synthetic images matching  the input text description. 
Recent works\cite{han2017stackgan,reed2016generative} has proposed to use a matching-aware loss to achieve this goal. The discriminator is used to differential fake image-text pair from real image-text pair. However, such loss complicate the task for discriminator because it does not only need to differentiate fake images from real images, it also need to match the text to images.
In this paper, we explicitly separate discriminator loss into 2 terms, one image loss is used to differentiating real/fake image, and 
one matching-aware loss is used to handle matching between images and text description. 


As is shown in Fig\ref{game}, a text description $t$ is first transformed into a text embedding $\psi(t)$ using an text encoder \cite{reed2016generative}. 

\textbf{Conditional augmentation: } Following the work of \cite{han2017stackgan}, instead of using the plan deterministic non-linear transformation of the origin al $\Psi(t)$, we obtain the conditional vector $c$ by sampling from a Gaussian distribution $\mathcal{N}(\mu(\psi(t)), \Sigma(\psi(t)) )$, in which the mean $\mu(\psi(t))$ and the diagonal covariance matrix $\Sigma(\psi(t))$ are non-linear transformation of the original text embedding $\psi(t)$. This type of conditional augmentation technique can be used to inject randomness to the text embedding vector $\psi(t)$ to mitigate the common problem that conventional conditional GAN training paradigm usually learns to $G$ that totally ignores the noise, resulting to results lacking diversity. In the spirit of variational auto-encoder, further Kullback-Leibler divergence regularization term  $D_{KL}(\mathcal{N}(\mu(\psi(t)), \Sigma(\psi(t)) )|| \mathcal{N}(0, \bm{I}))$ is added to the final loss function to push the Gaussian distribution to the norm distribution to address over-fitting.  In addition, this term might also play a role in balancing the $c$ and generated condition vector $c$ by forcing $c$ to have a similar scale to $z$. 

\subsection{Scalable Deeply Supervised}


\subsection{Functional Loss}
 
\begin{algorithm}
	\caption{Counting mismatches between two packed \DNA{} strings
		\label{alg:packed-dna-hamming}}
	\begin{algorithmic}[1]
		\Require{$x$ and $y$ are packed \DNA{} strings of equal length $n$}
		\Statex
		\Function{Distance}{$x, y$}
		\Let{$z$}{$x \oplus y$} \Comment{$\oplus$: bitwise exclusive-or}
		\Let{$\delta$}{$0$}
		\For{$i \gets 1 \textrm{ to } n$}
			\If{$z_i \neq 0$}
				\Let{$\delta$}{$\delta + 1$}
			\EndIf
		\EndFor
		\State \Return{$\delta$}
		\EndFunction
	\end{algorithmic}
\end{algorithm}



\subsection{Implementation Details}

\subsection{Network Architecture}

\section{Experiments}
\subsection{Evaluation metric}
\subsection{Quantitative and qualitative results}

\subsection{Case study}
\subsubsection{Arbitrary text description}
\subsubsection{Style transfer}

\section{Ablation study}
\subsubsection{loss study}
\subsubsection{architecture study}



{\small
\bibliographystyle{ieee}
\bibliography{reference_zizhao,egbib}
}

\end{document}
