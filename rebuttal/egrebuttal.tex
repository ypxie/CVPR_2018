\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array,booktabs,arydshln,xcolor} % for widening table line
\usepackage{amsthm,bm}
\usepackage{epstopdf}
\usepackage[flushleft]{threeparttable}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{tabularx} % in the preamble
\usepackage{float}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{capt-of}% or \usepackage{caption}
\usepackage{booktabs}
\usepackage{varwidth}
\usepackage{wrapfig}
\usepackage{makecell}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{2823} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Photographic Text-to-Image Synthesis with a Hierarchically-nested Adversarial Network}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}

\noindent
We would like to thank the three reviewers for their thoughtful review of the this manuscript and for their insightful comments and instructive suggestions, which help to improve quality of the manuscript. Our response follows.
%In brief, our paper presents a simple yet effective generative adversarial network (GAN) with hierarchically-nested discriminators to perform photographic text-to-image generation, which is end-to-end trainable and achieves state-of-the-art performance on three public datasets.

\noindent
\textbf{To Reviewer 1:} 1) \textit{Novelty of hierarchically nested discriminators:} Although the concept of `multi-resolution branch' has been used in several other fields mentioned by the reviewer and Related Works, but it's effectiveness has not been studied in GANs.
Training high-resolution GAN is known to be highly challenging. Previous methods resort to stacking a set of small GANs and train them progressively (see paragraph 4, Section 2). Differently, our method firstly shows that incorporating such hierarchical discriminators can overcome unsolved difficulties and assist the training of high-resolution GANs in an end-to-end manner.  Extensive experiment results demonstrate the superiority of our method.

%\textbf{To Reviewer 1:} 1) \textit{Novelty of hierarchically nested discriminators:} Although the concept of `multi-resolution branch' is used in several other fields, it is completely novel in GANs. Training high-resolution GAN is known to be highly challenge. Previous methods stack a set of small GANs and train them progressively (see paragraph 4, Section 2). Differently, our method firstly shows that incorporating such hierarchical discriminators can overcome unsolved difficulties and assist the training of high-resolution GANs in an end-to-end manner. 


\noindent
2) \textit{Additional citations and Missing details in Figure 2}: Thanks for pointing out the additional related works, we will cite them properly. We will also carefully revise the figure to make it intuitive and clear.

\noindent
3) \textit{Scope of text-to-image synthesis:} 
We focus on text-to-image synthesis as it is a relative new but key task in GANs, which already has plenty of difficulties that worth studies in a full paper. To be specific, achieving high diversity and semantic consistency simultaneously in high-resolution samples generation is a highly difficult problem even with provided charRNN text encodings. %And there are essential technical details to guarantee them in generator/discriminator designs. 
Nevertheless, as the reviewer indicated, our method looks universal to other type of image generation problems; We believe that exploiting other image generation tasks using our method will also be an interesting research area left open for the community. We will test them in the next version and open source our code to the community for future study.
we also want to emphasis that the used datasets are also large (especially for COO), representative and are capable of proving the effectiveness of our method.  In addition, the evaluation of text-to-image synthesis also needs rethinking. We propose the visual-semantic metric to alleviate labor evaluation used in [44] for  text-to-image synthesis evaluation.
%in complementary of the Inception-score and MS-SSIM for general GANs.


%3) \textit{Scope of text-to-image synthesis:} We focus on text-to-image synthesis as it is a relative new but key task in GANs, which already has plenty of difficulties that worth studies in a full paper. To be specific, keeping high diversity and semantic consistency simultaneously in high-resolution samples is non-trivial at all, although with provided charRNN encodings. And there are essential technical details to guarantee them in generator/discriminator designs. 
%The evaluation of text-to-image synthesis also needs rethinking, so we propose the visual-semantic metric in complementary of the Inception-score and MS-SSIM for general GANs. In addition, the used datasets are also large and representative (especially for COO). 
%Nevertheless, as the reviewer indicated, our method is definitely generic to other datasets. We will test them in the next version and also open source code for wide validation.

\noindent
\textbf{To Reviewer 2:} 1) \textit{Insufficient comparison: } Prog.GAN is currently recognized as one of the most effective methods to generate high-resolution images from noises. %GANs are evolving fast. 
Outperforming recent Prog.GAN already implies that our method outperforms much earlier LAPGAN (proposed in 2015). Moreover, LAPGAN only reports images of resolution up to $96^2$ , while our capability for high-resolution also demonstrates significant advantages. The cascaded refinement network is an image-to-image generative model with pixelwise semantic layout input, which is not relevant. Moreover, all of the three are actually not for text-to-image synthesis,% so are essential comparable methods. 
we discussed them in the related work as they share similar high-level motivations with ours. We have showed the state-of-the-art performance compared with most recent existing text-to-image synthesis methods on three datasets (including the best performance on the large COCO dataset) with three evaluation metrics.  The sufficiency and solidity of our experiments are highlighted by other two Reviewers as well. 

%%%Compared with previous related methods, we argue our experiment is sufficient to demonstrate the ability to generate high-resolution images.

\noindent
2) \textit{Comparison to [28]: } We appreciate the recommended new metrics. We tested our model on the CUB bird dataset following the procedure advised in [28] (i.e. query text, the inception model, and a bird word list used to match the ImageNet bird categories). Our method achieves a high (top-1) accuracy of 98.7 ($256^2$ images) compared with [28]'s $85\%$ (for a few bird category names in ImageNet do not have an exact match in the bird word list, we manually checked image-by-image). Particularly, less than ten images completely failed and the rest misclassified images still look like birds visually. We will complete the evaluation with this metric in the final version. The author ordering of [28] will be corrected. All results with source code will be released for wide test. 

% We believe this score further demonstrates the effectiveness of our method. 
\noindent
4) \textit{Qualitative results of failed cases}: 
We definitely agree with the reviewer that it is better to show some failure cases for completeness. We will try our best to compare and discuss this in the revision. In addition, Table 3 left compares class-wise MS-SSIM scores to StackGAN, which illustrates that our method does not  outperform StackGAN 100\% of the time. We hope this will also be helpful for the concerns. 

%Thanks for pointing this out. We will show more diverse qualitative results and discuss failure cases in the revision. Based on our observation, our method shows generally more realistic results, with clearly less sharp pixel transitions, more photographic colors, and nature saturability (see the supplementary material). And the evaluation results with three metrics also confirm our conclusion.

%We also refer the reviewer to the showed samples in our paper and in the StacGAN paper. You can easily feel differences. 
%The reason we did not add failure cases is that it is hard to explain why a certain case looks worse than another. But Table 3 left compares class-wise scores to StackGAN, which is helpful for the concerns. 

\noindent
3) \textit{About the style transfer loss:} 
We agree with the nice study in [28] that the perpetual loss is helpful to improve the classification accuracy.
But we also have the concern (as confirmed by our early experiment using intermediate representation of discriminator) that adding such strong loss might lead to model collapse and impede the diversity of the generated images, since balancing the generator and discriminator can be a very tricky task. However, we agree that finding an effective way to utilize the mentioned style transfer loss can be a future research topic that worth studies. 
 
%But we believe  this is an open question that worth future stduies.

%Please note, it is admitted that the inception score is not "perfect" and can not be the sufficient condition/proof of high-quality and semantic consistency images, considering the fact that CNNs (inception) can be "fooled" by only pixel changes without overall visual quality improvement. All-round improvement of qualitative results and other metrics is necessary. Our paper has a careful consideration and thorough demonstration for them.

%We agree with the nice study in [28] that the perpetual loss is helpful. 
%%Note that we investigate a simple vanilla-like GAN with hierarchically-nested discriminator to address this challenge. 
% We agree that adding semantic information in the intermediate outputs (like StackGAN), adding some style losses like [28], or even adding an inception-score loss directly, can push the inception score. But it not clear these approaches can enable end-to-end high-resolution GAN training as our method is capable of. And Investigation of them is out of our focus. Please note, it is admitted that the inception score is not "perfect" and can not be the sufficient condition/proof of high-quality and semantic consistency images, considering the fact that CNNs (inception) can be "fooled" by only pixel changes without overall visual quality improvement. All-round improvement of qualitative results and other metrics is necessary. Our paper has a careful consideration and thorough demonstration for them.

\noindent
\textbf{To Reviewer 3:} 1) \textit{Writing issues:} Thanks for pointing them out. 
The intuition behind VS-similarity is to increase the score $c$ (cosine similarity) between matching pairs while decrease these between the mismatching pairs.  We will definitely revise the manuscript to better explain the intuition and correct the misspelling of MS-SSIM.

\noindent
2) \textit{Choosing $R_i$:} The actual size of $R_i$ for discriminators is described in the Training and Architecture Details section in the supplementary material. Our preliminary experience is that choosing small $R_i$ at low resolution and a bit larger one at high resolution.

%Pls also see our supplementary material to check the clearly better visual results than StackGAN.



%\begin{table}[h] % retrieval
%	\small
%	\begin{center}
%		\begin{tabularx}{.33\textwidth}{c|cc}
%			\specialrule{1.5pt}{0pt}{0pt}  
%			\multirow{2}{*}{Method}    & \multicolumn{2}{c}{Dataset}    \\ \cline{2-3}
%						&     Bird        &    Flower              \\ \hline
%			[28]    	&    -/-/85.0    &       0.89         \\ \hline
%			StackGAN    &    \textbf{99.0}/-/69.1    &                 \\ \hline
%			HDGAN    &    \textbf{98.7}/-/84.2    &                 \\ \hline
%		
%		\end{tabularx} 
%	\end{center}
%	\vspace{-.4cm}
%	%\caption{} \vspace{-.3cm}
%\end{table}

%
%{\small
%\bibliographystyle{ieee}
%\bibliography{egbib}
%}

\end{document}
