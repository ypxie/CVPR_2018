\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{tabularx} % in the preamble
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\newcommand\VRule[1][\arrayrulewidth]{\vrule width #1}
\usepackage{array,booktabs,arydshln,xcolor} % for widening table line
\usepackage{amsthm,bm}
\usepackage{epstopdf}


\usepackage{amsthm}
\usepackage{multirow}

\usepackage{float}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{capt-of}% or \usepackage{caption}
\usepackage{booktabs}
\usepackage{varwidth}

\usepackage{algpseudocode,algorithm,algorithmicx}
\newcommand*\DNA{\textsc{dna}}

\newcommand*\Let[2]{\State #1 $\gets$ #2}
\algrenewcommand\algorithmicrequire{\textbf{Precondition:}}
\algrenewcommand\algorithmicensure{\textbf{Postcondition:}}


\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\usepackage{bm}

\usepackage[breaklinks=true,bookmarks=false]{hyperref}

%\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Semantic High-definition Image Synthesis \\ with a Hierarchically-nested Adversarial Network}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
This paper presents a novel  and effective approach to deal with the challenging task of generating synthetic photographic image conditioned on semantic image descriptions. Our method leverages the deep representations of convolutional layers and introduces hierarchical-nested adversarial training to regularize the representations to be compositional of multi-scale images. We present a new network architecture, which is intuitive as well as extensile, to push generated images to high resolutions with symmetric convolutional blocks.  We present a functional generative adversarial networks (GANs) training strategy to encourage more effective multimodal (i.e. image and text) information utilization in order to enhance multi-purpose adversary between real and fake distribution. To our best knowledge, our method is the first to show a fully end-to-end trainable network that can generate $256{\times}256$ images and is stackable to $512{\times}512$ resolution. With extensive experimental validation on three major datasets, our method significantly improves previous method and sets up a new state of the art to this field. 

\end{abstract}


%%%%%%%%% BODY TEXT
\section{Introduction}
Generating photographic high-resolution image conditional on arbitrary semantic description is an important problem in generative model research \cite{reed2016generative}. However, insufficient methods have been successfully developed to address this task due to its particular challenges. Text-to-image synthesis can be viewed as an special variation of conventional generative models, which aim to translate a low-dimensional embedding  to a much more complex RGB image space.  Differently, the embedding space of the former is induced by semantic descriptions. Therefore, this task requires the generated images to be \textit{semantically consistent}, i.e., the generated images not only preserve the principal concept of descriptions generally but also preserve the fine-grained descriptive details in image pixels. 

Generative adversarial networks (GANs) have become the main solution to this task. 
Reed \etal \cite{reed2016generative} address this task through a GAN based framework. But this method only handles image up to $64{\times}64$ resolution and usually can barely generate vivid object details.
Based on \cite{reed2016generative}, Zhang \etal \cite{han2017stackgan} present a successful approach (StackGAN) by stacking another GAN to generate high-quality $256{\times}256$ images given low resolution $64{\times}64$ synthetic images. This method needs two-stage training to achieve the goal. Later on, Dong \etal \cite{dong2017semantic} 
bypasses the difficult of translate vector embedding to RGB images and treat it as an pixel-to-pixel translation \cite{isola2016image}, by re-rendering an arbitrary-style training ($128{\times}128$) image conditioned on a targeting semantic description. Unfortunately, its high resolution synthesis ability is still unclear. 
At present, how to train from the low-dimensional embedding to synthesize high resolution (e.g. $256{\times}256$) and diverse images in an end-to-end manner is still an open and attractive question. 

We outline several empirical observations for this challenges using GANs, particularly for text-to-image synthesis. The first comes from a fundamental difficulty of balancing the convergence between generators and discriminators. There are a rich number of literature tacking this issue \cite{salimans2016improved,huang2016stacked}. For text-to-image synthesis, how to utilize the multimodal information between images and text in discriminator is extremely important to generate effective gradients and needs careful consideration. The second is that huge pixel space in high resolution images \cite{han2017stackgan}. 
An effective regularization to reduce the variances in generators is critical \cite{huang2016stacked}. 
Moreover, keeping semantic consistency and diversity in generated images needs more specialized designs in generators to better make use of gradients from discriminators. 


%The first reason is that the pixel space in high resolution image is substantially large \cite{han2017stackgan}. 
% Keeping semantic consistency as well as diversity
%The first reason comes from the fundamental difficulty of balancing the stability between generators and discriminators. There are a rich number of literature trying to stabilize GAN training \cite{salimans2016improved} and regularize networks \cite{huang2016stacked}. 
%But especially for text-to-image synthesis, how to utilize the multimodal information between images and text in discriminator is extremely important and needs careful consideration. The second reason is that the pixel space in high resolution image is substantially large \cite{han2017stackgan}. Keeping semantic consistency as well as diversity in generated images needs more specialized designs for discriminators to generate effective gradients, as well as generators to prevent gradient vanishing. 

With careful consideration of these outlined problems, in this paper, we propose a novel method that can directly generate high resolution and semantic consistent images trained in a fully end-to-end manner. The contributions are described follows.

To tackle the problem of space leap from text and images, our method leverages the hierarchical representations of convolutional layers through a deeply-nested layer constraint, which is simply motivated by two intuitions. 
We hope feature maps in different layers are disentangled RGB image space, which can linearly represent targeting images at corresponding resolutions and maximize the implicit effectiveness of feature representations.
We introduce accompanying hierarchically-nested adversarial objective at intermediate layers to achieve this goal. To better cooperate with discriminators, we present an quite intuitive network structure design to encourage more efficient and effective training.

To tackle the instable training of GANs, 
\textcolor{red}{we propose to enforce discriminators at different resolutions can differentiate real/fake image, correct/incorrect image-text pairs, and real/fake image pairs jointly with multiple losses.} We will show that this multi-purpose conditional strategy is mutually beneficial to encourage them only focus on respect duties. It is a regularization to limit the discriminators to use unexpected information to make decision. 

We have validated our proposed method on three datasets, i.e. CUB birds \cite{}, Oxford-102 flowers \cite{}, and large-scale COCO datasets \cite{}. Extensive experimental results and analysis demonstrate the effectiveness of our method and significantly improved performance compared against previous state of the arts. All source code will be released.


\section{Related Work}
Deep generative models grows wide interests recently, including GANs \cite{goodfellow2014generative}, Variational Auto-encoders (VAE) \cite{kingma2013auto}, etc \cite{oord2016pixel}. 
There are substantial existing methods investigating better usage of GANs for different applications, such as image synthesis \cite{radford2015unsupervised, shrivastava2016learning}, (unpaired) pixel-to-pixel translation \cite{isola2016image,zhu2017unpaired},  medical applications \cite{costa2017towards}, etc \cite{ledig2016photo,huang2016stacked}.

Text-to-image synthesis not only requires diverse and high-quality generation but also requires precise semantically consistent mapping in the image space.  Reed \etal \cite{reed2016generative} introduce the first method that can generate $64{\times}64$ resolution images, which is similar with DCGAN \cite{radford2015unsupervised}. This method presents a new strategy to image-text matching aware adversarial training. Reed \etal \cite{reed2016learning} propose generative
adversarial what-where network (GAWWN) to enable "where and what" instructions in text-to-image synthesis, which uses extra information to help generate $128{\times}128$ resolution images. StackGAN \etal \cite{han2017stackgan} propose to a two-stage stacking GAN training approach that is able to generate $256{\times}256$ compelling results. Recently, Dong \etal \cite{dong2017semantic} propose to learn a joint embedding of images and text so as to re-render a prototype image conditioned on targeting descriptions. Cha \etal \cite{char2017perceptual} explore the usage of the perceptional loss with a pretrained ImageNet CNN \cite{johnson2016perceptual} and Dash \etal \cite{dash2017tac} make use of auxiliary classifiers (similar with \cite{odena2016conditional}) to assist GAN training. 
	
Learning a continuous mapping from low-dimensional embeddings to complex real data distribution is a long-standing problem. Although GANs have made significant progressive, there are still many  unsolved difficulties, e.g. training instability and high resolution. Wide methods have been proposed to address those tasks, through various stabilization training techniques \cite{salimans2016improved,arjovsky2017wasserstein,berthelot2017began,shrivastava2016learning,odena2016conditional}, regularization using outside knowledge (e.g. image labels, ImageNet CNNs, etc) \cite{dosovitskiy2016generating,ledig2016photo,dash2017tac,dash2017tac}. While note that our method does not use any extra information apart from training paired text and images. Moreover, it is easy to see the training difficulty grows significantly as targeting image resolution increases.

%for example, label smoothing and discriminator feature matching \cite{salimans2016improved, }, training stronger discriminator is beneficial to encourage generators \cite{arjovsky2017wasserstein}. Balancing through a equilibrium enforcing method \cite{berthelot2017began}
%Preventing discriminators from forgetting past samples and re-introduces artifacts \cite{shrivastava2016learning}. Increasing the information richness in GANs has been shown very useful \cite{odena2016conditional}.  

To synthesize high resolution image, cascade networks play an important role to decompose original difficult tasks to multiple subtasks.
Denton \etal \cite{denton2015deep} trains a cascade of GANs within a Laplacian pyramid framework (LAPGAN) and use each to generate difference images, conditioned on random noises and output from last level of the pyramid, to push up output resolution through by-stage refinement. StackGAN also shares similar strategy with LAPGAN. Following this strategy, Chen \etal \cite{chen2017photographic} present a cascaded refinement networks to synthesis high-resolution scene from semantic maps. 
Huang \etal \cite{huang2016stacked}
shows a top-down stack GAN to leverage mid-level representations, which shares some similarities with StackGAN and our method. However, this method need multiple symmetric  bottom-up discriminators and pre-training of discriminators, and the usage for high-resolution image is unclear \cite{han2017stackgan}. Compared with these strategies that train low- to high-resolution GANs progressively, our method has advantages of utilizing multi-level representations to encourage implicit subtask integration, which enables end-to-end high resolution image synthesis.

%More advantages of our method compared with others will be demonstrated in the following.

Leveraging hierarchical representations of neural networks is an effective way to enhance implicit multi-scaling and ensembling for tasks such as image classification \cite{lee2015deeply} and pixel or object classification \cite{xie2015holistically,cai2016unified,long2015fully}. DSN \cite{lee2015deeply} proposes deep supervision in hierarchical convolutional layers to increase the discriminativeness of feature representations. 
Our method is partially inspired by supervised CNNs with deep supervision \cite{lee2015deeply,xie2015holistically}. Our proposed hierarchically-nested adversarial supervision enhances layer representations to support final high resolution outputs.   
\begin{figure}[t]
	\centering
	%	\includegraphics[width=0.48\textwidth]{}
	\caption{} \label{fig:archs}
\end{figure}

%
%
%
%\subsection{Summarize different architectures}
%refer to HED.
\begin{figure}[t]
	\centering
%	\includegraphics[width=0.48\textwidth]{}
	\caption{} \label{fig:archs-compare}
\end{figure}

<<<<<<< HEAD
\section{Method}
Figure \ref{fig:archs} illustrate the overall method.

\subsection{Architecture Design}
Our proposed network, composed by a generator and a discriminator, is symmetric as well as extensile. We introduce the details as well as motivations.

\textbf{Generator} The generator is simpled composed by three kinds of modules, termed as $K$-repeat res-blocks, stretching layers, and linear compression layers.
A single res-block in the $K$-repeat res-block is a standard residual block  \cite{he2016identity} containing two convolutional (conv) layers (with batch normalization (BN) \cite{ioffe2015batch} and ReLU). The stretching layer serves to reduce feature map size and dimension. It simply contains an scale-$2$ nearest up-sampling layer followed by a convolutional layer with BN+ReLU. And the linear compression layer is a conv layer followed by a Tan, whose compact design enforces feature maps in conv blocks can linearly represent RGB images at arbitrary scales.
Starting from a $1024{\times}4{\times}4$ embeddings replicated by a $1024$-d text embedding, the generator simples use $M$ $K$-repeat res-blocks connected by $M{-}1$ in-between stretching layers until the feature maps reach to the targeting resolution. So for $256{\times}256$ targeting resolution with $K{=}1$, there are $M{=}6$ $1$-repeat res-blocks and $5$ stretching layers. Such simple and symmetric design makes our generator helpful for gradient flows. We will verify in experiments. 
With a predefined side-output scales $\{S_i\}$, we apply the compression layer at those scales to produce synthetic images.

\textbf{Discriminator} The discriminator simply contains consecutive stride-2 conv layers with BN and LeakyReLU \cite{} (the depth depends on the input image size) until the feature map has $512{\times}4{\times}4$ dimension. There two branches are added on top of it for multi-purpose discriminator design (see next section) . One is a direct $4{\times}4$ conv layers to produce a 2-d vector to classify real and fake. Another one first concatenate $128{\times}4{\times}4$ text embedding (replicated from a $128$-d text embedding). Then we use an $1{\times}1$ conv to fuse text and image information; it is critical to guarantee the semantic consistency in the final results. Finally, a $4{\times}4$ conv layer produces a 2-d vector.

All intermediate conv layers use $3{\times}3$ kernels (with reflection padding and no bias). We remove ReLU after the skip-addition of each residual block, with an intention to reduce sparse gradients. 
We also experimented other normalization (i.e. instance normalization \cite{ulyanov2016instance} and layer normalization \cite{ba2016layer}) used by \cite{zhu2017unpaired,chen2017photographic}. Both are not satisfactory. 

=======
>>>>>>> 38cae0ba4268ba013c981ad0847366af08cd0ab4
\section{Preliminary}
\subsection{Generative Adversary Network}
Generative adversary networks (GANs) are composed of a generator $G$ and a discriminator $D$, which are alternatively trained to compete with each other in a two-player minimax game. The discriminator $D$ is optimized to distinguish the synthesized images from the real images, meanwhile, the generator $G$ is trained to fool the discriminator by trying to convert an input distribution $p_z$ to the true data distribution $p_{data}$. Concretely, the overall training procedure can be analogous to the following two-player min-max game $V(D, G)$,
\begin{equation}
\label{game}
\begin{split}
\underset{G}{\min}\ \underset{D}{\max}\ V(D, G) =  &\mathbb{E}_{x\sim p_{data}}[\log D(x)] +  \\
&\mathbb{E}_{z\sim p_{z}}[\log (1-D(G(z)))]		   
\end{split}
\end{equation}
where $x$ is a real image sampled from the true data distribution $p_{data}$, $z$ is a noise vector usually sampled from some predefined distribution $p_{z}$ (e.g., Gaussian distribution).
In practice, It's been shown to work better for the generator $G$ to minimizing $-\log(D(G(z)))$ rather than $\log(1-D(G(z)))$, namely, the $\log(D)$ trick.

Conditioned GAN \cite{isola2016image} is a variation of traditional GAN where both the generator network $G$ and discriminator network $D$ are conditioned on an additional condition vector besides the noise vector. In this paper, we consider the task of training a deep  generative adversarial network conditioned on the embedding of the text descriptions of the images.

\textbf{Advantages} Figure \ref{fig:archs-compare} illustrates the architecture difference between widely-used cascade strategy \cite{han2017stackgan,denton2015deep} and our strategy. Compared with it, 
\\

\section{Method}
Figure \ref{fig:archs} illustrate the overall method.
%To generate high-resolution photo-realistic images from text embeddings, we propose a simple and effective Hierarchically-nested Generative Adversarial Network.
Our approach is to train both generator and discriminator conditioned on sentence embedding vectors. We leverage functional loss for discriminator and hierarchically nested side outputs as hierarchical supervision for both generator and discriminator to achieve our goal of generating photo-realistic images from text descriptions.  In general, the generator takes text embedding and random noise vector as input and synthesis images at different scales (e.g, $64\times64$, $128\times128$, $256\times256$). The discriminator, on the other hand, is trying to judge the pair of synthesized images and the corresponding text embedding as fake pairs as well as differentiating the real images from the fake images. $G$ and $D$ are optimized alternatively until converges.


\textbf{Notation } In this paper, we use the following notations. Denote the image dimension, text embedding dimension and the dimension of the noise input as $I$, $T$ and $Z$, respectively. Please note that, $G$ has multiple branches and generates multiple images of different sizes and for each image size, we have a distinct $D$. For simplicity, we only describe one image size.  

The generator is denoted $G: \mathcal{R}^{Z}: \times \mathcal{R}^{T}\rightarrow\mathcal{R}^{I}$, the image discriminator $D_{img}: \mathcal{R}^{I}\rightarrow\{0, 1\}$, and the matching-aware pair discriminator $D_{pair}: \mathcal{R}^{I}: \times \mathcal{R}^{T}\rightarrow\{0, 1\}$. 


\textbf{Variational text embedding } Following the work of \cite{han2017stackgan}, instead of using the plan deterministic non-linear transformation of the original $\Psi_t$, we obtain the conditional vector $c$ by sampling from a Gaussian distribution $\mathcal{N}(\mu(\psi_t), \Sigma(\psi_t) )$, in which the mean $\mu(\psi_t)$ and the diagonal covariance matrix $\Sigma(\psi_t)$ are non-linear transformation of the original text embedding $\psi_t$. This type of conditional augmentation technique can be used to inject randomness to the text embedding vector $\psi_t$ to mitigate the common problem that conventional conditional GAN training paradigm usually learns to totally ignore the randomness, resulting to collapsing results lacking diversity \cite{reed2016generative}. In addition, using this kind of text embedding makes the input pure stochastic, making it more difficult for the generator to over fit the training data.



\subsection{Architecture Design}
Our proposed network, composed by a generator and a discriminator, is symmetric as well as extensile. We introduce the details as well as motivations.

\textbf{Generator} The generator is simpled composed by three kinds of modules, termed as $K$-repeat res-blocks, stretching layers, and linear compression layers.
A single res-block in the $K$-repeat res-block is a standard residual block  \cite{he2016identity} containing two convolutional (conv) layers (with batch normalization (BN) \cite{ioffe2015batch} and ReLU). The stretching layer serves to reduce feature map size and dimension. It simply contains an scale-$2$ nearest up-sampling layer followed by a convolutional layer with BN+ReLU. And the linear compression layer is a conv layer followed by a Tan, whose compact design enforces feature maps in conv blocks can linearly represent RGB images at arbitrary scales.
Starting from a $1024{\times}4{\times}4$ embeddings replicated by a $1024$-d text embedding, the generator simples use $M$ $K$-repeat res-blocks connected by $M{-}1$ in-between stretching layers until the feature maps reach to the targeting resolution. So for $256{\times}256$ targeting resolution with $K{=}1$, there are $M{=}6$ $1$-repeat res-blocks and $5$ stretching layers. Such simple and symmetric design makes our generator helpful for gradient flows. We will verify in experiments. 
With a predefined side-output scales $\{S_i\}$, we apply the compression layer at those scales to produce synthetic images.

\textbf{Discriminator} The discriminator simply contains consecutive stride-2 conv layers with BN and LeakyReLU \cite{} (the depth changes according to the input size) until the feature map has $512{\times}4{\times}4$ dimension. There two branches are added on top of it for multi-purpose discriminator design (see next section) . One is a direct $4{\times}4$ conv layers to produce a 2-d vector to classify real and fake. Another one first concatenate $128{\times}4{\times}4$ text embedding (replicated from a $128$-d text embedding). Then we use an $1{\times}1$ conv to fuse text and image information; it is critical to guarantee the semantic consistency in the final results. Finally, a $4{\times}4$ conv layer produces a 2-d vector.

All intermediate conv layers use $3{\times}3$ kernels (with reflection padding and no bias). We remove ReLU after the skip-addition of each residual block, with an intention to reduce sparse gradients. 
We also experimented other normalization (i.e. instance normalization \cite{ulyanov2016instance} and layer normalization \cite{ba2016layer}) used by \cite{zhu2017unpaired,chen2017photographic}. Both are not satisfactory. 




\subsection{Scalable Deeply Supervised}
Instead of only predicting the high resolution in the final output, our model is designed to predict multi-level images of gradually increasing dimension (e.g., 256$\times$256 as the final output with 64$\times$64 and 128$\times$128 being the side output). 
 
In principle, the lower-resolution side output is forced to learn the rough structure (e.g., object sketch, color information and background) of the images, the subsequent higher-resolution path is used to add more details to the images. 

The lower-level side output works by regularizing the lower-level image feature. Since the model is trained in an end-to-end fashion, we can observe more consistent information exhibited in both lower and higher-resolution outputs than stackGan\cite{han2017stackgan}. This type of side output is also proven useful because it allows short path between image supervision information to the hidden feature maps, which could potentially helps training.

\subsection{Generator Loss}
As is shown in Fig\ref{game}, In the generator $G$, we first sample a random vector $z\in \mathcal{R}^{Z} \scriptsize{\sim} \mathcal{N}(0,1)$, then text embedding vector $\psi_{t}$ is obtained by applying an per-trained text encoder on the image description $t$, in this paper, we use the char-rnn based text encoder provided in \cite{reed2016generative}. 

Conditioned on variational text embedding $c$ and the random vector $z$, a synthetic image $\hat{x}$ is generated as $\hat{x}\leftarrow G(c,z)$, it is simply a feed forward pass of the generator. The generator $G$ consists of stacks of residual blocks, bilinear sampling layer is used to up-sample the feature maps. 

The generator's goal is to generator image that match the input text embedding and looks as real as possible. So the loss function for our model mainly consists of two parts, namely, matching aware loss and one image loss.  

Given a set of chosen input pair $(x, \psi_t)$, the generator is optimizing the following function
 \begin{equation}
 \label{genloss}
 \begin{split}
 \mathcal{L}_G = & \mathbb{E}_{z\sim p_{z}, t \sim p_{data}}[-\log( D_{sent}( G(z, c_t),  \psi_t) ) ] + \\
				  & \mathbb{E}_{z\sim p_{z}, t \sim p_{data}}[-\log( D_{img}( G(z, c_t) ) ) ] + \\
                  & D_{KL}(\mathcal{N}(\mu(\psi_t), \Sigma(\psi_t) )|| \mathcal{N}(0, \bm{I})) 		   
 \end{split}
 \end{equation}
 where $c_t$ is the variational conditional vector generated from $\mathcal{N}(\mu(\psi_t), \Sigma(\psi_t) )$. To make the network differentiable, in practice, we adopted the re-parametric trick\cite{vae}, where $c_t$ is actually computed as $\mu(\psi_t)+\Sigma(\psi_t)\odot \epsilon$, where $\epsilon\in \mathcal{R}^{T}$ is draw from a normal Gaussian distribution.  
 
 In the spirit of variational auto-encoder, further Kullback-Leibler divergence regularization term  $D_{KL}(\mathcal{N}(\mu(\psi_t), \Sigma(\psi_t) )|| \mathcal{N}(0, \bm{I}))$ is added to the final loss function to push the Gaussian distribution to the norm distribution to address over-fitting.  In addition, this term might also play a role in balancing the $c_t$ and generated condition vector $c_t$ by forcing $c$ to have a similar scale to $z$.  In the following sections, we denote $c_t$ as the variational conditional text vector.
 
 
\subsection{Functional Loss of Discriminator}

As is shown in Figure. \ref{overview}, the discriminator $D$ in our model mainly consists of two branches: matching-aware pair discrimination branch $D_{pair}$, and image discrimination branch $D_{img}$, both branches share the same  image encoder.

Given a pair of image and text embedding $(x, \psi{t})$ as input, in the discriminator $D$, we first transfer $x$  to an image code $Code_x$. $Code_x$ along with $f_{sent}(\psi_t)$ is concatenated and fed to the matching aware branch to compute a final scale value indicating the probability of the pair being real or fake, where $f_{sent}$ consists of one linear layer and one leaky ReLU transformation,  the matching-aware branch is a set of residual blocks with strid-2 convolution, more details of the architecture will be provided in the appendix. Besides that, the image code is also fed to an image-disc branch, whose structure is very similar to the matching-aware branch, to judge the probability of the input image being real or fake. We performance batch normalization on all convolutional layers. \textcolor{red}{like stackgan, I will add much more architecture details later}  

The generator $G$ and discriminator $D$ are optimized alternatively using their corresponding loss function. 

 Our goal is to generate images that not only look as realistic as real images, but also matching the given text description.
 Therefore, the discriminator must handle errors of the following two categories: real image with mismatch conditional text information, fake image with real text. In order to achieve this, the previously proposed matching aware discriminator is explicitly trained with three pair configurations, namely: real image with right text as positive training sample, and the aforementioned two sources of error pairs as negative training samples. 
 
 Given a mini-batch of training data consists of images $x$ and the corresponding text $t$, mismatching text $\hat{t}$. First of all, we sample fakes images using $G$.
 $\hat{x_1} = G(z_1, \psi_t)$ and $\hat{x_2} = G(z_2, \psi_t)$, where $z_1$ and $z_2$ are random vector sampled from norm Gaussian distribution. 
  
The loss function of the matching aware discriminator are summarized as:
  \begin{equation}
  \label{matchignloss}
  \begin{split}
  \mathcal{L}_D = 	  &\mathbb{E}_{(x, t) \sim p_{data}}[\log( D( x  \psi_t) ) ] -  0.5*(\\
					  &\mathbb{E}_{(x, \hat{t}) \sim p_{data}}[\log(D(x, \psi_{\hat{t}} )  ] +\\
					  &   \mathbb{E}_{t \sim p_{data}}[\log(D(\hat{x_1}, \psi_t) ) ] 
  \end{split}
  \end{equation}
This loss explicit force the discriminator to differentiate correct pair from the wrong pair.  However, there is no explicit loss that push the discriminator to differentiate real images from the fake images. The way that they formulate all the task into pair based solution complicate the task of the generator. In this paper, we propose to directly add another loss that concentrate the image quality only beside the matching loss. Specifically, it can be given by:
  \begin{equation}
  \label{discoss}
  \begin{split}
  \mathcal{L}_D = 	&\mathbb{E}_{(x, t) \sim p_{data}}[\log( D( x  \psi_t) ) ] -  0.5*(\\
		  &\mathbb{E}_{(x, \hat{t}) \sim p_{data}}[\log(D_{pair}(x, \psi_{\hat{t}} )  ] +\\
		  &\mathbb{E}_{t \sim p_{data}}[\log(D_{pair}(\hat{x_1}, \psi_t) ) ] + \\
		  &\mathbb{E}_{x \sim p_{data}}[\log( D_{img}(x) )] - 0.5* ( \\
		  &\mathbb{E}[\log( D_{img}(\hat{x_1}))] + \mathbb{E}[\log( D_{img}(\hat{x_2}) )  ] )
  \end{split}
  \end{equation}
\textcolor{red}{equations needs to be verified, just a rough place holder}

Further more, this separation of the image loss from the matching loss open further possibilities, such as utilizing the unlabeled image to improve both the genrator and discriminator. \textcolor{red}{maybe utilizing the imagenet for training the coco, or save as another paper.}


 \begin{algorithm}
 	\caption{Multi scale GAN training algorithm with step size $\alpha$, number of iteration $S$, we summarize the algorithm using SGD for simplicity.\textcolor{red}{need to change to goodfellow algorithm style}}
 	\label{CHalgorithm}
 	\begin{algorithmic}[1]
 		
 		\For{$i \gets 1 \textrm{ to } S$ }
 		\State $z_1, z_2 \sim \mathcal{N}(0,\bm{1})^Z$
 		\State $\hat{x_1}, \hat{x_2} = G(z_1, \psi_t),  G(z_2, \psi_{\hat{t}})$ %\{Forward through $G$ using matching text embedding\}
 		\State $d_r$ = $D_{pair}(x, \psi_t)$ \{(real image, matching text)\}
 		\State $d_w$ = $D_{pair}(x, \psi_{\hat{t}})$ \{(real image, wrong text)\}
 		\State $d_f$ = $D_{pair}(\hat{x_1}, \psi_t)$ \{(fake image, matching text)\}
 		\State $s_r, s_1, s_2$ = $D_{img}(x), D_{img}(\hat{x_1})$, $D_{img}(\hat{x_1})$ %\{Forward through $D_{img}$ with $\hat{x_1}$\}
 		
 		\State $loss_{img}$ = $ (\log(1-s_1) + \log(1-s_2))/2 $
 		\State $loss_{pair}$ = $ (\log(1-d_w) + \log(1-d_f))/2 $
 		
 		\State $\mathcal{L}_{D}$ = $\log(d_r) + \log(s_r) +loss_{img} + loss_{pair}$
 		\State $\mathcal{L}_{G}$ = $\log(1-d_f) +  ( \log(1-s_1) + \log(1-s_2) )/2 $
 		\State $\theta_G$ = $\theta_G$ - ${\delta \mathcal{L}_G } / {\delta \theta_G}$  \{Update generator\}
 		\State $\theta_D$ = $\theta_D$ - ${\delta \mathcal{L}_D} / {\delta \theta_D}$  \{Update generator\}
 		
 		\EndFor
 		
 	\end{algorithmic}
 \end{algorithm}
 


 
%\begin{algorithm}
%	\caption{Multi scale GAN training algorithm with step size $\alpha$, number of iteration $S$, we summarize the algorithm using SGD for simplicity.   
%		\label{alg:gan-train}}
%	\begin{algorithmic}[1]
%		\Input{sample a minibatch of images, $x$, matching text description $t$, mis-matching $\hat{t}$,
%			}
%		\For{$i \gets 1 \textrm{ to } S$}
%			$z_1, z_2 \sim \mathcal{N}(0,\bm{1})^Z$
%			\State $\hat{x_1}$=$G(z_1, \psi_t)$ \Comment{Forward through $G$ using matching text embedding}
%			\STATE {$\hat{x_2}$}{$G(z_2, \psi_\hat{t})$} \Comment{Forward through $G$ using mis-matching text embedding}
%			
%			\STATE {$d_r$}{$D_{pair}(x, \psi_t)$} \Comment{(real image, matching text)}
%			\STATE {$d_m$}{$D_{pair}(x, \psi_\hat{t})$} \Comment{(real image, mis-matching text)}
%			\STATE {$d_f$}{$D_{pair}(\hat{x_1}, \psi_t)$} \Comment{(fake image, matching text)}
%			\STATE {$s_r$}{$D_{img}(x)$} \Comment{Forward through $D_{img}$ with $x$}
%			\STATE {$s_1$}{$D_{img}(\hat{x_1})$} \Comment{Forward through $D_{img}$ with $\hat{x_1}$}
%			\STATE {$s_2$}{$D_{img}(\hat{x_1})$} \Comment{Forward through $D_{img}$ with $\hat{x_2}$}
%			\STATE {$\mathcal{L}_{D}$}{$(d_r-1)^2 + (s_r-1)^2 + (d_m + d_f + s_1 + s2)/2 $} 
%			\STATE {$\mathcal{L}_{G}$}{$((d_f -1)^2 + (s_1 -1)^2 + (s_2 -1)^2 $} 
%			\STATE {$\theta_G$}{$\frac{\delta \mathcal{L}_G}{\delta \theta_G}$}  \Comment{Update generator}
%			\STATE {$\theta_D$}{$\frac{\delta \mathcal{L}_D}{\delta \theta_D}$}  \Comment{Update generator}
%		\EndFor	
%	\end{algorithmic}
%\end{algorithm}


\subsection{Implementation Details}

<<<<<<< HEAD
=======

\section{Experiments}

\subsection{Dataset}
\textbf{Dataset} We use three datasets for evaluation. \textcolor{red}{Yuanpu address}

\subsection{Evaluation metric}
\textbf{Inception score }

\subsection{Baseline}
>>>>>>> 38cae0ba4268ba013c981ad0847366af08cd0ab4

In this section, we evaluate the proposed method both qualitatively and quantitatively on three public datasets. We denote our method as HDGAN, referring as High-definition results as well as the design of hierarchically-nested discriminators.


%We mainly compare to the state-of-the art StackGAN \cite{han2017stackgan} for text-to-image synthesis and demonstrate substantially improvement. 



\textcolor{red}{Yuanpu address}

\begin{table}[t] % retrieval
	\begin{center}
		\begin{tabularx}{.477\textwidth}{c|ccc}
			\specialrule{1.5pt}{0pt}{0pt}  
			\multirow{2}{*}{Method}	& \multicolumn{3}{c}{Dataset}	\\ \cline{2-4}
							 		&	 CUB		&	Oxford  & COCO		     \\ \hline
			GAN-INT-CLS 	&	$2.88{\pm}.04$		& 	$2.66{\pm}.03$		& $7.88{\pm}.07$	 \\
			GAWWN 	  &		$3.60{\pm}.07$		&     -      &          - \\ 
			StackGAN     &		$3.70{\pm}.04$	&	 $3.20{\pm}.01$			&  $8.45{\pm}.03$		\\ 
		%	StackGAN$^\star$     &		$3.89{\pm}.05$	&	 $3.16{\pm}.01$			&  $8.45{\pm}.03$		\\ \hline
			TAC-GAN	 &	-		&		$\bm{3.45{\pm}.05}$		& -	\\	\hline
			HDGAN 		&	$\bm{4.01{\pm}.06}$	&	$ \bm{3.45{\pm}.07}$			&  -  \\ \hline
		\end{tabularx} \vspace{-.4cm}
	\end{center}
	\caption{The inception-score evaluation on three datasets. The higher score reflects more meaningful synthetic images and higher diversity. The proposed HDGAN outperforms others significantly.} \label{table:score}
\end{table}

\begin{table}[t] % retrieval
	\begin{center}
		\begin{tabularx}{.42\textwidth}{c|ccc}
			\specialrule{1.5pt}{0pt}{0pt}  
			Scale 						& StackGAN & HDGAN & HR Test					\\ 				\hline
            $64{\times}64$              &   ${2.95{\pm}.02}$       &  	$\bm{3.46{\pm}.04}$ &  ${3.46{\pm}.04}$	\\
           $128{\times}128$             &   ${3.35{\pm}.02}$      &  $\bm{3.85{\pm}.04}$	& ${3.45{\pm}.04}$	\\
           $256{\times}256$             &   $3.70{\pm}.04$     &	$\bm{4.01{\pm}.06}$ & ${3.46{\pm}.04}$ \\ \hline
		\end{tabularx}
	\end{center} \vspace{-.4cm}
	\caption{Scale accuracy \textcolor{red}{changing to figure is better}.} \label{table:deep-nest}
\end{table}

\begin{figure}[t]
	\centering
	%	\includegraphics[width=0.48\textwidth]{}
	\caption{Results on CUB} \label{fig:vis-cub}
\end{figure}
\begin{figure}[t]
	\centering
	%	\includegraphics[width=0.48\textwidth]{}
	\caption{Results on Oxford} \label{fig:vis-oxford}
\end{figure}

%
\subsection{Comparative Results}
We compare our results with GAN-INT-CLS \cite{reed2016generative}, GAWWN \cite{reed2016learning}, and  StackGAN \cite{han2017stackgan}, and TAC-GAN \cite{dash2017tac}. StackGAN is currently the state of the art method in general that is comprehensively validated. StackGAN \cite{han2017stackgan} generates synthetic images up to $256{\times}256$ resolution. We show that our method can generates images up to  $512{\times}512$.

Table \ref{table:score} shows the quantitative evaluation. HDGAN achieves significantly improvement compared to other methods. For example, HDGAN improves GAN-INT-CLS by ${\sim}30.1\%$  and ${\sim}8.4\%$ on CUB.



\subsection{Case study}
\textbf{Generalization to real user test}
We test the generalization of our method by asking a user to write a sentence 


\subsubsection{Style transfer}
\textcolor{red}{Yuanpu address}

\begin{table}[t] % retrieval
	\begin{center}
		\begin{tabularx}{.268\textwidth}{ccc|c}
			\specialrule{1.5pt}{0pt}{0pt}  
			\multicolumn{3}{c|}{Components}	&  \multirow{2}{*}{Score}	\\ \cline{1-3}
				 64	& 128	& 256 			& 		\\ \hline
					&  		&	\checkmark	&	${3.52{\pm}.04}$	\\ 
						&  	\checkmark	&	\checkmark	&		\\
				\checkmark	&  			&	\checkmark	&  ${4.14{\pm}.03}$		\\
				\checkmark	&  \checkmark		&	\checkmark	&	${4.01{\pm}.04}$ \\

		\end{tabularx}
	\end{center} \vspace{-.4cm}
	\caption{Ablation study for hierarchically-nested adversarial supervision.} \label{table:deep-nest}
\end{table}


\section{On the effects of Individual Components}
\subsection{Hierarchically-nested adversary}
Recall that the proposed hierarchically-nested adversarial supervision plays a role of regularizing the layer representations. In practice, we apply it on feature maps at $\{64, 128, 256\}$. In Table \ref{table:deep-nest}, we show the inception score by removing partial of supervision components. As can be observed, ...

StackGAN emphasizes the importance of using text embeddings in the (stage-II) mid-level features of the $256{\times}256$ generator by showing an large drop from $3.7$ to $3.45$ without doing so. The text embedding plays an important role in maintaining the diversity and semantic consistency in StackGAN. While in our method, we only use text embeddings at the input. Our improved results demonstrate that our hierarchically-nested adversarial supervision regularizes the generator to achieve the same goal. 


\subsection{Multi-purpose discriminator}
\textcolor{red}{Yuanpu address}

\begin{figure}[t]
	\centering
	%	\includegraphics[width=0.48\textwidth]{}
	\caption{} \label{fig:vallina-res}
\end{figure}


\subsection{Design principles}
Note that for text-to-image synthesis, StackGAN is the only successful method to generate $256{\times}256$ images.
At the same time, it also shows the difficulty (impossibility) of directly training a vanilla $256{\times}256$ GAN, which fails to generate meaningful images. 
We test this extreme case using our method by removing all nested supervisions (the first row of Table \ref{table:deep-nest}). 
Our method actually can still generate fairly good. Figure \ref{fig:vallina-res} shows the qualitatively results. This strongly proves the effectiveness of our designs in generators and discriminator losses.

Initially, we tried to share top layers of the hierarchical discriminators of HDGAN inspired by \cite{liu2017unsupervised} with an intuition to reduce their variances and unify their common goal (i.e. differentiates real and fake despite difficult scales). However, we did not find any benefits from this and our independent discriminators can be well-trained by themselves. 

BN is known to be problematic for .... 



\section{Conclusion}
...

{\small
\bibliographystyle{ieee}
\bibliography{reference_zizhao,egbib}
}

\end{document}
